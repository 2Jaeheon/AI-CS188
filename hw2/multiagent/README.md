# Q1: Reflex Agent 개선
ReflexAgent의 평가 함수를 구현하여 팩맨이 유령을 피하면서 음식을 잘 먹도록 만듦

- 여러가지 사항을 기준으로 현재 무슨 행동을 해야할까를 선택함.

### 고려 요소
- 팩맨과 음식 간의 거리
- 팩맨과 가장 가까운 유령 간의 거리
- 팩맨과 가장 가까운 캡슐 거리
- 겁먹은 유령과의 거리
- 남은 음식 개수
- STOP의 여부

다음과 같이 모든 데이터를 통해 score를 갱신하도록 함.
즉, 다음에 행동할 모든 action들에 대해서 시뮬레이션을 해보고 특징을 통한 점수를 계산
점수가 가장 높은 행동을 한다.

### 장점
- 단순하지만 매우 직관적이고 효과적이다.
- w(가중치) 조정을 통해서 안정적인 선택이 가능함.

### 어려운 점 및 해결
- 처음의 특징을 뽑아낼 때 어떤 함수를 써야할 지 몰라 game.py의 코드를 찾아서 필요한 함수를 꺼냄
- 각 점수를 게산할 때 계산이 안 되는 현상에서 분모가 0이면 안 되는 현상을 발견할 할 수 있어 문모에 1을 더하는 방향으로 진행.
- 가중치를 갱신할 때마다 문제가 하나씩 발생함. 예를 들어서 캡슐과 하나의 포인트 사이에서 계속 점수를 먹지 않고 계속 근처에 맴도는 현상이 발견됨. 이는 전체 개수에 관한 가중치를 더 크게 조정하여 해결하였음. 즉, 개수가 줄어들면 더 점수를 조금 잃어버림 따라서 더 유리한 방향



# Q2: Minimax Agent 구현
팩맨과 유령들을 포함하는 multi-agent minimax 알고리즘 구현

- 팩맨은 MAX, 적은 MIN
- 트리의 depth는 팩맨과 적(유령)이 한 번씩 움직이는 단위를 기준으로 함
- Helper를 이용해서 재귀적으로 구현해야함
    - 재귀적으로 구성한다는 것은 각 에이전트가 다음에 할 수 있는 모든 행동에 관해 minimax를 다시 수행하여 갈 수 있는 모든 행동 중 가장 좋은 방향으로 가겠다는 것임.

### 구현 함수
#### getAction(gameState): Goal!!!
- 초기 가능한 모든 action을 생성하고 각 상태에 관해 minimax를 호출해줌.

#### minimax(state, agent, depth)
- 깊이에 도달했거나, 게임 승/패가 정해졌다면 평가함수를 반환해줌.
- Pacman의 차례에는 maxEvaluate() 함수를 호출
- Ghost의 차례에는 minEvaluate() 함수를 호출

#### maxEvaluate(state, depth)
- agent == 0일 때
- legal Action을 반복하면서 minimax를 재귀적으로 호출 -> 가장 높은 점수를 반환해줌.

#### minEvaluate(state, agent, depth)
- agent > 0 일 때
- agent > 0 인 이유는 ghost가 여러마리일 가능성이 있기 때문임. 
- 따라서 모든 ghost에 대해 조사하고 다시 pacman의 turn으로 넘어가야 함.
- legal Action을 모두 반복하여 minimax를 재귀적으로 호출
- 가장 낮은 점수를 반환

### 어려운 점 및 해결 
- 초기에 재귀에 관해 동작 원리가 이해가 가지 않아서 여러 번 개념을 학습하여 해결함.

- 초기 getAction() 함수에서 왜 agent = 0으로 했을 때 안 되는 이유와 1로 해야하는 이유에 대해서 알 수 없었음. 하지만, getAction()에서 팩맨이 한 번 행동을 취한 상태를 넘기기 때문에 agent = 1이 되어야 오류가 발생하지 않음.

- STOP을 제외하려고 legalActions에서 filtering을 시도하였으나 autograder에서 모두 실패하는 상황이 발생함. 이는 케이스에서 STOP인 상황도 모두 고려해서 만들어진 구조이기 때문이라고 추측됨.


# Q3: Alpha-Beta Agent 구현
minimax 알고리즘에 알파-베타 가지치기 추가하여 탐색 효율화

- Pruning을 통해서 불필요한 상태 탐색을 방지함.
- 상태 순서 변경 없이 구현

# Q4: Expectimax Agent 구현
유령들이 최적이 아닌 랜덤하게 움직일 수 있다는 가정을 고려한 Expectimax 알고리즘 구현

- 적(유령)은 min이 아닌 기대값을 기반으로 행동함
- 모든 적(유령)은 uniform probability로 행동한다고 가정

# Q5: Better Evaluation Function 구현
이전까지 구현한 agent들이 사용할 수 있는 상태 기반 평가 함수를 설계

- 행동이 아닌 상태를 평가
- 음식, 유령, 캡슐 등 여러 요소를 고려
