# Q1: Reflex Agent 개선
ReflexAgent의 평가 함수를 구현하여 팩맨이 유령을 피하면서 음식을 잘 먹도록 만듦

- 여러가지 사항을 기준으로 현재 무슨 행동을 해야할까를 선택함.

### 고려 요소
- 팩맨과 음식 간의 거리
- 팩맨과 가장 가까운 유령 간의 거리
- 팩맨과 가장 가까운 캡슐 거리
- 겁먹은 유령과의 거리
- 남은 음식 개수
- STOP의 여부

다음과 같이 모든 데이터를 통해 score를 갱신하도록 함.
즉, 다음에 행동할 모든 action들에 대해서 시뮬레이션을 해보고 특징을 통한 점수를 계산
점수가 가장 높은 행동을 한다.

### 장점
- 단순하지만 매우 직관적이고 효과적이다.
- w(가중치) 조정을 통해서 안정적인 선택이 가능함.

### 어려운 점 및 해결
- 처음의 특징을 뽑아낼 때 어떤 함수를 써야할 지 몰라 game.py의 코드를 찾아서 필요한 함수를 꺼냄
- 각 점수를 게산할 때 계산이 안 되는 현상에서 분모가 0이면 안 되는 현상을 발견할 할 수 있어 문모에 1을 더하는 방향으로 진행.
- 가중치를 갱신할 때마다 문제가 하나씩 발생함. 예를 들어서 캡슐과 하나의 포인트 사이에서 계속 점수를 먹지 않고 계속 근처에 맴도는 현상이 발견됨. 이는 전체 개수에 관한 가중치를 더 크게 조정하여 해결하였음. 즉, 개수가 줄어들면 더 점수를 조금 잃어버림 따라서 더 유리한 방향



# Q2: Minimax Agent 구현
팩맨과 유령들을 포함하는 multi-agent minimax 알고리즘 구현

- 팩맨은 MAX, 적은 MIN
- 트리의 depth는 팩맨과 적(유령)이 한 번씩 움직이는 단위를 기준으로 함
- Helper를 이용해서 재귀적으로 구현해야함
    - 재귀적으로 구성한다는 것은 각 에이전트가 다음에 할 수 있는 모든 행동에 관해 minimax를 다시 수행하여 갈 수 있는 모든 행동 중 가장 좋은 방향으로 가겠다는 것임.

### 구현 함수
#### getAction(gameState): Goal!!!
- 초기 가능한 모든 action을 생성하고 각 상태에 관해 minimax를 호출해줌.

#### minimax(state, agent, depth)
- 깊이에 도달했거나, 게임 승/패가 정해졌다면 평가함수를 반환해줌.
- Pacman의 차례에는 maxEvaluate() 함수를 호출
- Ghost의 차례에는 minEvaluate() 함수를 호출

#### maxEvaluate(state, depth)
- agent == 0일 때
- legal Action을 반복하면서 minimax를 재귀적으로 호출 -> 가장 높은 점수를 반환해줌.

#### minEvaluate(state, agent, depth)
- agent > 0 일 때
- agent > 0 인 이유는 ghost가 여러마리일 가능성이 있기 때문임. 
- 따라서 모든 ghost에 대해 조사하고 다시 pacman의 turn으로 넘어가야 함.
- legal Action을 모두 반복하여 minimax를 재귀적으로 호출
- 가장 낮은 점수를 반환

### 어려운 점 및 해결 
- 초기에 재귀에 관해 동작 원리가 이해가 가지 않아서 여러 번 개념을 학습하여 해결함.

- 초기 getAction() 함수에서 왜 agent = 0으로 했을 때 안 되는 이유와 1로 해야하는 이유에 대해서 알 수 없었음. 하지만, getAction()에서 팩맨이 한 번 행동을 취한 상태를 넘기기 때문에 agent = 1이 되어야 오류가 발생하지 않음.

- STOP을 제외하려고 legalActions에서 filtering을 시도하였으나 autograder에서 모두 실패하는 상황이 발생함. 이는 케이스에서 STOP인 상황도 모두 고려해서 만들어진 구조이기 때문이라고 추측됨.


# Q3: Alpha-Beta Agent 구현
minimax 알고리즘에 알파-베타 가지치기 추가하여 탐색 효율화

- Pruning을 통해서 불필요한 상태 탐색을 방지함.
- 상태 순서 변경 없이 구현

### 구현 과정
minimax 함수를 구현했기 때문에 이를 바탕으로 여기에 alpha, beta값을 기반으로 pruning을 하는 과정을 추가하면 됨.<br>
alpha 값과 beta값을 통해서 value가 있을 때 이를 max / min에 따라 pruning 함<br>

alpha: 현재까지 탐색된 max 노드에서의 최적 값(가장 높은 값)
beta: 현재까지 탐색된 min 노드에서의 최적 값(가장 낮은 값)
<br>

팩맨은 가장 높은 값을 선택해야함.<br>
-> 어떤 특정 value가 존재할 때 지금까지 탐색된 min 노드에서 나온 노드보다 크다면 해당 경로는 탐색하지 않아도 됨<br>
-> pruning 함
<br>

고스트는 가장 낮은 값을 선택해야함.<br>
-> 유령이 선택한 점수는 팩맨이 선택한 (max node) 점수보다 value가 낮다면 팩맨은 해당 경로를 절대 선택하지 않을 것<br>
-> pruning 함

### 어려운 점 및 해결
- 처음에 alpha, beta가 어떤 에이전트에서 사용되고, 어떻게 동작하는 지에 관해 헷갈렸음<br>
-> 루트에서 Pacman이 alpha를 기준으로 움직이고, Ghost는 beta 기준으로 움직인다는 것을 명확히 이해하고, 각 단계에서 어떤 기준으로 값을 비교할지 주석과 함께 정리하며 재설계함

- getAction에서 alpha값을 갱신해야 하는데, 이를 하지 않아서 코드가 정상적으로 작동하지 않았음<br>
-> getAction에서 팩맨은 가장 높은 점수를 선택하고자 함. 따라서 alpha값을 갱신해서 다음 action에 대해서 평가를 해야 정상적으로 재귀적인 동작이 일어남


# Q4: Expectimax Agent 구현
유령들이 최적이 아닌 랜덤하게 움직일 수 있다는 가정을 고려한 Expectimax 알고리즘 구현

- 적(유령)은 min이 아닌 기대값을 기반으로 행동함
- 모든 적(유령)은 uniform probability로 행동한다고 가정

### 구현 과정
- 이는 minimax Agent를 구현했을 때와 마찬가지로 동일한 로직을 사용함.
- 하지만 다른 것은 minEvaluate() 대신 expectationEvaluate()를 통해서 기대값을 기반으로 문제를 해결해야함.
<br>
즉, 팩맨과 Ghost의 역할은 다음과 같음
- Pacman (MAX agent): 가장 높은 점수를 내는 방향으로 행동
- Ghosts (Chance agent): 모든 행동을 동일한 확률로 선택 (uniform probability)
<br>
따라서 기존의 함수에서 평균을 내서 반환을 하는 형식으로만 수정하였음.


# Q5: Better Evaluation Function 구현
이전까지 구현한 agent들이 사용할 수 있는 상태 기반 평가 함수를 설계

- 행동이 아닌 상태를 평가
- 음식, 유령, 캡슐 등 여러 요소를 고려

### 구현 과정
- Q1번과 다르게 상태를 기반으로 평가를 해야함.
- 따라서 다양한 Feature들을 통해서 점수를 계산하였음.

feature를 계산하기 위해 사용한 지표는 다음과 같음
- 남은 음식의 개수: 적을수록 유리
- 음식과의 거리: 가까울수록 유리
- 유령과의 거리: 멀수록 유리
- 겁먹은 유령과의 거리: 짧을수록 유리
- 겁먹은 유령의 개수: 많을수록 유리
- 캡슐 거리: 짧을수록 유리
- 푸드 클러스터링: 음식까지의 평균 거리를 나타내며, 작을수록 유리
- 유령과의 안전 거리: 2영역 내의 유령의 거리이며, 작을수록 유리

이를 통해서 코드를 게산하였고, 가중치를 계속해서 수정해 나가는 과정을 거쳤음.

### 어려운 점 및 해결
- 평가를 하기 위해 가중치를 튜닝하는 것이 매우 어려웠음

- 팩맨이 움직이지 않고 가만히 있는 상황이 자주 발생하였음
